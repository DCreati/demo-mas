Neural Architecture Search via Differentiable Graph Optimization
Recent advances in neural architecture search (NAS) have automated the design of neural networks, but most approaches remain computationally expensive, requiring thousands of GPU hours. This paper introduces DiffGraphNAS, a novel differentiable approach to architecture search that operates directly on the computational graph representation of neural networks. Unlike previous gradient-based NAS methods that optimize continuous relaxations of discrete choices, our approach maintains structural integrity by treating the architecture search as a graph optimization problem with differentiable node operations and edge connections.
The key innovation of DiffGraphNAS lies in its graph-theoretic formulation, where each candidate operation is represented as a learnable weighted node, and connections between operations form a directed acyclic graph (DAG). We employ a novel gradient estimation technique based on Gumbel-Softmax relaxation combined with path-wise gradient flow, enabling efficient end-to-end training. Our method introduces three significant contributions: (1) a multi-scale graph encoding that captures both local operation efficiency and global network topology, (2) a resource-aware objective function that jointly optimizes accuracy, latency, and model size, and (3) a progressive search strategy that dynamically adjusts the search space complexity during training.
We evaluate DiffGraphNAS on image classification benchmarks including CIFAR-10, CIFAR-100, and ImageNet. Our method discovers architectures that achieve 97.2% accuracy on CIFAR-10 with 3.1M parameters, outperforming manually designed networks and previous NAS methods while reducing search time to just 4 GPU hours on a single V100. On ImageNet, our discovered architecture achieves 78.4% top-1 accuracy with only 5.2M parameters, competitive with EfficientNet-B0 but with 40% fewer FLOPs. Ablation studies demonstrate that each component of our approach contributes to both search efficiency and final architecture quality.
The theoretical analysis shows that DiffGraphNAS converges to locally optimal architectures under mild assumptions about operation smoothness. We provide extensive experimental validation showing that our graph-based formulation naturally handles architecture constraints such as memory limits and latency budgets, making it suitable for deployment on resource-constrained devices. Furthermore, our approach exhibits strong transferability, with architectures discovered on CIFAR-10 transferring effectively to other vision tasks including object detection and semantic segmentation.
However, several limitations remain. First, the search space is currently limited to convolutional operations, and extending to transformer-based architectures requires further investigation. Second, while our method is significantly faster than previous NAS approaches, it still requires careful hyperparameter tuning to balance exploration and exploitation. Third, the discovered architectures sometimes contain irregular patterns that complicate hardware optimization. Future work will address these limitations by incorporating hardware-aware cost models directly into the search objective and exploring mixed convolutional-transformer search spaces.
In conclusion, DiffGraphNAS demonstrates that treating architecture search as a differentiable graph optimization problem enables efficient discovery of high-performing neural networks. Our graph-theoretic perspective provides a principled framework for incorporating structural constraints and resource budgets into the search process, paving the way for practical automated machine learning systems that can adapt to diverse computational environments and task requirements.